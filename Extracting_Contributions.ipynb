{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0996d8",
   "metadata": {},
   "source": [
    "# Extraction of Contribution Sentences from PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4843c4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\prash\\anaconda3\\lib\\site-packages (from PyPDF2) (4.1.1)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "639a5df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.23.6-cp39-none-win_amd64.whl (3.5 MB)\n",
      "Collecting PyMuPDFb==1.23.6\n",
      "  Downloading PyMuPDFb-1.23.6-py3-none-win_amd64.whl (24.5 MB)\n",
      "Installing collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.23.6 PyMuPDFb-1.23.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\prash\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e04afbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Fine-grained Interpretability Evaluation Benchmark for Neural NLP.pdf\n",
      "A Multilingual Bag-of-Entities Model for Zero-Shot Cross-Lingual Text Classification.pdf\n",
      "An Alignment-based Approach to Text Segmentation Similarity Scoring.pdf\n",
      "Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models.pdf\n",
      "Characterizing Verbatim Short-Term Memory in Neural Language Models.pdf\n",
      "Cognitive Simplification Operations Improve Text Simplification.pdf\n",
      "Collateral facilitation in humans and language models.pdf\n",
      "Combining Noisy Semantic Signals with Orthographic Cues Cognate Induction for the Indic Dialect Continuum.pdf\n",
      "Computational cognitive modeling of predictive sentence processing in a second language.pdf\n",
      "Continual Learning for Natural Language Generations with Transformer Calibration.pdf\n",
      "Detecting Unintended Social Bias in Toxic Language Datasets.pdf\n",
      "Enhancing the Transformer Decoder with Transition-based Syntax.pdf\n",
      "Entailment Semantics Can Be Extracted from an Ideal Language Model.pdf\n",
      "How Hate Speech Varies by Target Identity A Computational Analysis.pdf\n",
      "Incremental Processing of Principle B Mismatches Between Neural Models and Humans.pdf\n",
      "Leveraging a New Spanish Corpus for Multilingual and Cross-lingual Metaphor Detection.pdf\n",
      "On Language Spaces, Scales and Cross-Lingual Transfer of UD Parsers.pdf\n",
      "On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation.pdf\n",
      "OpenStance Real-world Zero-shot Stance Detection.pdf\n",
      "Optimizing text representations to capture (dis)similarity between political parties.pdf\n",
      "Parsing as Deduction Revisited Using an Automatic Theorem Prover to Solve an SMT Model of a Minimalist Parser.pdf\n",
      "PIE-QG Paraphrased Information Extraction for Unsupervised Question Generation from Small Corpora.pdf\n",
      "Probing for targeted syntactic knowledge through grammatical error detection.pdf\n",
      "Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL).pdf\n",
      "Shared knowledge in natural conversations can entropy metrics shed light on information transfers.pdf\n",
      "Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities.pdf\n",
      "That’s so cute! The CARE Dataset for Affective Response Detection.pdf\n",
      "Towards More Natural Artificial Languages.pdf\n",
      "Visual Semantic Parsing From Images to Abstract Meaning Representation.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import fitz\n",
    "\n",
    "#function to Identify contribution sentences\n",
    "def identify_contribution_sentences(sentence):\n",
    "    pronouns = ['we', 'this paper', 'in this paper', 'i', 'the authors', 'our', 'the study', 'the research']\n",
    "    verbs = ['propose', 'contribute', 'introduce', 'describe', 'present', 'report', 'discuss', 'employ', 'study',\n",
    "             'explore', 'aim', 'hypothesize', 'argue', 're-use', 'evaluate', 'compare', 'baseline', 'result',\n",
    "             'approach', 'method', 'extend', 'ensemble', 'transfer learning', 'architecture', 'neural method',\n",
    "             'translation systems', 'system development', 'domain adaptation', 'train', 'experiment']\n",
    "\n",
    "    sentence_lower = sentence.lower()  # Convert the sentence to lowercase for case-insensitive matching\n",
    "\n",
    "    words = sentence_lower.split()  # Split the sentence into words\n",
    "    if words:\n",
    "        for pronoun in pronouns:\n",
    "            if words[0].startswith(pronoun):  # Check if the first word matches a pronoun\n",
    "                for verb in verbs:\n",
    "                    if verb in words:  # Check if any word in the sentence matches a verb\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "'''\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        text = ''\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "'''\n",
    "#fetching text from PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred while processing {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "#Process the PDf files to extract contribution and Non Contribution sentences\n",
    "def process_pdf_files_in_folder(folder_path):\n",
    "    contribution_sentences_all = []\n",
    "    non_contribution_sentences_all = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            #Opening and reading PDF file\n",
    "            if file.endswith(\".pdf\"):\n",
    "                print(file)\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                pdf_text = extract_text_from_pdf(pdf_path)\n",
    "                sentences = pdf_text.split('.')\n",
    "                \n",
    "                contribution_sentences = []\n",
    "                non_contribution_sentences = []\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    if identify_contribution_sentences(sentence):\n",
    "                        contribution_sentences.append(sentence.strip().replace('\\n', ''))\n",
    "                        #contribution_sentences.append(sentence)\n",
    "                    else:\n",
    "                        non_contribution_sentences.append(sentence.strip().replace('\\n', ''))\n",
    "                        #non_contribution_sentences.append(sentence)\n",
    "\n",
    "                contribution_sentences_all.extend(contribution_sentences)\n",
    "                non_contribution_sentences_all.extend(non_contribution_sentences)\n",
    "\n",
    "    # Create DataFrames\n",
    "    contribution_df = pd.DataFrame({'text': contribution_sentences_all})\n",
    "    non_contribution_df = pd.DataFrame({'text': non_contribution_sentences_all})\n",
    "    non_contribution_df=non_contribution_df.head(contribution_df.shape[0])\n",
    "    contribution_df['label']=1\n",
    "    non_contribution_df['label']=0\n",
    "    Research_df=pd.concat([contribution_df,non_contribution_df])\n",
    "    Research_df=Research_df.sample(frac=1)\n",
    "    Research_df['year']='2022'\n",
    "    Research_df['conference']='CoNLL'\n",
    "    Research_df.to_csv('all_contribution_sentences_2022.csv', index=False)\n",
    "\n",
    "# Replace 'your_folder_path' with the path to your folder containing PDFs\n",
    "process_pdf_files_in_folder('C://Users//prash//CoNLL_final//2022')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e43e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run above code for each year and combine all the years to create final Dataset using below code\n",
    "df1=pd.read_csv('all_contribution_sentences_2013.csv')\n",
    "df2=pd.read_csv('all_contribution_sentences_2014.csv')\n",
    "df3=pd.read_csv('all_contribution_sentences_2015.csv')\n",
    "df4=pd.read_csv('all_contribution_sentences_2016.csv')\n",
    "df5=pd.read_csv('all_contribution_sentences_2017.csv')\n",
    "df6=pd.read_csv('all_contribution_sentences_2018.csv')\n",
    "df7=pd.read_csv('all_contribution_sentences_2019.csv')\n",
    "df8=pd.read_csv('all_contribution_sentences_2020.csv')\n",
    "df9=pd.read_csv('all_contribution_sentences_2021.csv')\n",
    "df10=pd.read_csv('all_contribution_sentences_2022.csv')\n",
    "df_final=pd.concat([df1, df2, df3,df4,df5,df6,df7,df8,df9,df10], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a0dad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('all_contribution_sentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a33662f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contribution Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We propose a multilingual bag-of-entities (M-\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We evaluate the performance of the M-BoE\\nmode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We train the model using training data in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our contributions are as follows:\\n•We present...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Our method successfully improves the per-\\nfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Our method dynami-\\ncally injects entity infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In particular, the perfor-\\nmance of our model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We attribute this to the num-\\nber of entities...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We at-\\ntribute this to the ability of our met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We achieved state-of-the-art results on\\nthree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We would also like to inves-\\ntigate whether o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Contribution Sentences\n",
       "0   We propose a multilingual bag-of-entities (M-\\...\n",
       "1   We evaluate the performance of the M-BoE\\nmode...\n",
       "2   We train the model using training data in the ...\n",
       "3   Our contributions are as follows:\\n•We present...\n",
       "4   Our method successfully improves the per-\\nfor...\n",
       "5   Our method dynami-\\ncally injects entity infor...\n",
       "6   In particular, the perfor-\\nmance of our model...\n",
       "7   We attribute this to the num-\\nber of entities...\n",
       "8   We at-\\ntribute this to the ability of our met...\n",
       "9   We achieved state-of-the-art results on\\nthree...\n",
       "10  We would also like to inves-\\ntigate whether o..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a47ef7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contribution Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We propose a multilingual bag-of-entities (M-\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We evaluate the performance of the M-BoE\\nmode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We train the model using training data in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our contributions are as follows:\\n•We present...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Our method successfully improves the per-\\nfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Our method dynami-\\ncally injects entity infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In particular, the perfor-\\nmance of our model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We attribute this to the num-\\nber of entities...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We at-\\ntribute this to the ability of our met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We achieved state-of-the-art results on\\nthree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We would also like to inves-\\ntigate whether o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Contribution Sentences\n",
       "0   We propose a multilingual bag-of-entities (M-\\...\n",
       "1   We evaluate the performance of the M-BoE\\nmode...\n",
       "2   We train the model using training data in the ...\n",
       "3   Our contributions are as follows:\\n•We present...\n",
       "4   Our method successfully improves the per-\\nfor...\n",
       "5   Our method dynami-\\ncally injects entity infor...\n",
       "6   In particular, the perfor-\\nmance of our model...\n",
       "7   We attribute this to the num-\\nber of entities...\n",
       "8   We at-\\ntribute this to the ability of our met...\n",
       "9   We achieved state-of-the-art results on\\nthree...\n",
       "10  We would also like to inves-\\ntigate whether o..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribution_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef031806",
   "metadata": {},
   "source": [
    "# Combing Contribution sentences from other conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "905be0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_conll=pd.read_csv('all_contribution_sentences.csv')\n",
    "df_jlmr=pd.read_csv('C://Users//prash//Downloads//Sujit-JMLR.csv')\n",
    "df_emnlp=pd.read_csv(\"C://Users//prash//Downloads//EMNLP.csv//EMNLP.csv\")\n",
    "df_final_contributions=pd.concat([df_conll,df_jlmr,df_emnlp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2756b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_contributions.to_csv(\"final_contributions.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8669e674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text          9955\n",
      "label            6\n",
      "year             3\n",
      "conference       3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "missing_values_per_column = df_final_contributions.isnull().sum()\n",
    "\n",
    "print(missing_values_per_column)\n",
    "df_final_contributions.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c488974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375267, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_contributions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7b8b36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text  label    year  \\\n",
      "0       In addition tothe ﬁve comparison methods used ...    1.0  2013.0   \n",
      "3       In this paper, we present a system that combin...    1.0  2013.0   \n",
      "4       In addition, the selec-tion of classifiers, fe...    1.0  2013.0   \n",
      "5       This paper proposes a boosting algorithm fora ...    0.0  2013.0   \n",
      "6       We also compare to HTK,a ﬁxed-structure HMM wi...    1.0  2013.0   \n",
      "...                                                   ...    ...     ...   \n",
      "385215  It is suggested that such a\\nvolume is enough ...    1.0  2022.0   \n",
      "385216  In contrast,\\nlong paragraph inputs describe t...    1.0  2022.0   \n",
      "385218  Feature selection is often useful in boosting ...    0.0  2022.0   \n",
      "385221  We compare models in terms of ofﬂine\\nNLU perf...    1.0  2022.0   \n",
      "385224  In addition to the distilled baseline, we also...    1.0  2022.0   \n",
      "\n",
      "                                              conference  \\\n",
      "0                                                  CoNLL   \n",
      "3                                                  CoNLL   \n",
      "4                                                  CoNLL   \n",
      "5                                                  CoNLL   \n",
      "6                                                  CoNLL   \n",
      "...                                                  ...   \n",
      "385215  Empirical Methods in Natural Language Processing   \n",
      "385216  Empirical Methods in Natural Language Processing   \n",
      "385218  Empirical Methods in Natural Language Processing   \n",
      "385221  Empirical Methods in Natural Language Processing   \n",
      "385224  Empirical Methods in Natural Language Processing   \n",
      "\n",
      "                                             cleaned_text  \\\n",
      "0       in addition tothe ﬁve comparison method used b...   \n",
      "3       in this paper we present a system that combine...   \n",
      "4       in addition the selection of classifier featur...   \n",
      "5       this paper proposes a boosting algorithm forum...   \n",
      "6       we also compare to htka ﬁxedstructure hmm with...   \n",
      "...                                                   ...   \n",
      "385215  it is suggested that such a volume is enough t...   \n",
      "385216  in contrast long paragraph input describe the ...   \n",
      "385218  feature selection is often useful in boosting ...   \n",
      "385221  we compare model in term of ofﬂine nlu perform...   \n",
      "385224  in addition to the distilled baseline we also ...   \n",
      "\n",
      "                                           cleaned_tokens  sentence_length  \\\n",
      "0       [addit, toth, ﬁve, comparison, method, use, al...              131   \n",
      "3       [paper, present, system, combinesa, set, stati...               94   \n",
      "4       [addit, select, classifi, featur, train, sampl...               87   \n",
      "5       [paper, propos, boost, algorithm, fora, semima...               55   \n",
      "6       [also, compar, htka, ﬁxedstructur, hmm, three,...               88   \n",
      "...                                                   ...              ...   \n",
      "385215  [suggest, volum, enough, train, gpl, model, ne...               58   \n",
      "385216  [contrast, long, paragraph, input, describ, ma...               84   \n",
      "385218  [featur, select, often, use, boost, drift, det...               50   \n",
      "385221  [compar, model, term, ofﬂin, nlu, perform, als...               94   \n",
      "385224  [addit, distil, baselin, also, cre, ate, anoth...              101   \n",
      "\n",
      "        word_count  unique_word_count  avg_word_length  \n",
      "0               18                 18         6.333333  \n",
      "3               13                 12         6.307692  \n",
      "4               13                 13         5.769231  \n",
      "5                7                  7         7.000000  \n",
      "6               13                 13         5.846154  \n",
      "...            ...                ...              ...  \n",
      "385215          11                 11         4.363636  \n",
      "385216          12                 12         6.083333  \n",
      "385218           8                  8         5.375000  \n",
      "385221          16                 16         4.937500  \n",
      "385224          15                 13         5.800000  \n",
      "\n",
      "[219000 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display cleaned text DataFrame\n",
    "print(df_final_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb8c2c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove text less than 50 characters\n",
    "def filter_text(text):\n",
    "    if len(str(text)) >= 50:\n",
    "        return text\n",
    "    else:\n",
    "        return None  # Replace shorter text with None/NaN\n",
    "\n",
    "# Apply the function to the specified column\n",
    "df_final_contributions['cleaned_text'] = df_final_contributions['cleaned_text'].apply(filter_text)\n",
    "\n",
    "# Drop rows with None/NaN values in the column\n",
    "df_final_contributions.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08f2f559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219000, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_contributions.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
